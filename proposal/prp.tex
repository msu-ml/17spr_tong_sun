\documentclass[acmtog, authorversion]{acmart}

\usepackage{booktabs} % For formal tables

\usepackage[ruled]{algorithm2e} % For algorithms
\renewcommand{\algorithmcfname}{ALGORITHM}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\SetAlCapHSkip{0pt}
\IncMargin{-\parindent}

% Metadata Information
% \acmJournal{TOG}
% \acmVolume{9}
% \acmNumber{4}
% \acmArticle{39}
% \acmYear{2010}
% \acmMonth{3}

% Copyright
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}

% DOI
% \acmDOI{0000001.0000001_2}

% Paper history
% \received{February 2007}
% \received{March 2009}
% \received[final version]{June 2009}
% \received[accepted]{July 2009}


% Document starts
\begin{document}
% Title portion
\title{Adaptive Smooth Rectifier}
\author{Mengying Sun}
\orcid{1234-5678-9012-3456}
\affiliation{%
  \institution{Michigan State University}
  \department{Department of Computer Science and Engineering}
  \streetaddress{104 Jamestown Rd}
  \city{East Lansing}
  \state{MI}
  \postcode{00000}
  \country{USA}}
\author{Xiaoran Tong}
\affiliation{%
  \institution{Michigan State University}
  \department{Department of Epidemiology and Biostatistics}
  \streetaddress{909 Fee Rd}
  \city{East Lansing}
  \state{MI}
  \postcode{48824}
  \country{USA}
}

\renewcommand\shortauthors{Zhou, G. et al}

\begin{abstract}
Through this project we explore an alternative form of the rectifier activation units of adaptive smoothness, and, with such alternative, the possibility of relatively lightweighted networks out perfroming the mainstream deep networds under problem domains other than image classification.
\end{abstract}


%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below. 
%
% \begin{CCSXML}
% <ccs2012>
%  <concept>
%   <concept_id>10010520.10010553.10010562</concept_id>
%   <concept_desc>Computer systems organization~Embedded systems</concept_desc>
%   <concept_significance>500</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>10010520.10010575.10010755</concept_id>
%   <concept_desc>Computer systems organization~Redundancy</concept_desc>
%   <concept_significance>300</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>10010520.10010553.10010554</concept_id>
%   <concept_desc>Computer systems organization~Robotics</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>10003033.10003083.10003095</concept_id>
%   <concept_desc>Networks~Network reliability</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
% </ccs2012>  
% \end{CCSXML}

% \ccsdesc[500]{Computer systems organization~Embedded systems}
% \ccsdesc[300]{Computer systems organization~Redundancy}
% \ccsdesc{Computer systems organization~Robotics}
% \ccsdesc[100]{Networks~Network reliability}

%
% End generated code
%

% We no longer use \terms command
\terms{AI, Algorithms, Performance}

\keywords{ReLU}


\thanks{This work is supported by the National Science Foundation,
  under grant CNS-0435060, grant CCR-0325197 and grant EN-CS-0329609.

  Author's addresses: G. Zhou, Computer Science Department, College of
  William and Mary; Y. Wu {and} J. A. Stankovic, Computer Science
  Department, University of Virginia; T. Yan, Eaton Innovation Center;
  T. He, Computer Science Department, University of Minnesota; C.
  Huang, Google; T. F. Abdelzaher, (Current address) NASA Ames
  Research Center, Moffett Field, California 94035.}


\maketitle


\section{Introduction}
The popular trend in the filed of Machine Learning has always been pursuing a ``deeper'' and ``bigger'' Artificial Neural Network (ANN), known as Deep Learning, ever since the revival of the classic Multiple Layered Perceptron (MLP) around 2006 \cite{DL:Intro1}. Although the topology of an MLP and its capability of emulating functions of any complexity is well known for decades, going towards depth of tens and hundreds of layers was made possible by the greedy unsupervised pre-training \cite{DL:Intro1}, aided by the improvement in computation power and an explosion of data. Thansk to the introduction of rectified linear unit (ReLU) as a replacement of the classical sigmoid neurons \cite{DL:Relu1}, recently networks do not have to go through the pre-training procedure, and has been able to push their depth to a new high, at times even more than a thousand layers. As the current dominant trend, ReLU offers a number of advantages \cite{DL:Relu1}. From the the perspective of neural computation, the asymmetric, half dominant activation curve can better mimics the actual biological activation of the neurons in the central neural system (CNS), plus, the network as whole respects the fact that only a small fraction the CNS is active at any moment\cite{DL:Relu1}. From the function searching point of view, ReLU promotes model sparsity at neurons level that helps to reduce overfit, which means more room for deeper structures given the same tolerance of total complexity (e.g., the number of active nodes and connections at any time). ReLU also partially solved the issue of diminishing gradient when the large input signal is given to the traditional sigmoid or tangent activation units, which also elevates the potential of going ``deeper''. 

Despite many victory scored by deep ANN \cite{DL:Intro2, DL:Intro3}, going deeper does not come without a cost. Aside from the computation load and the memory constraint of the GPUs, the network still tends to overfit when the depth keeps growing, a recent trend is to randomly ``dropout'' a portion of neurons at any update cycle to promote each neuron extract different features, which improves generalization performce by a large margin \cite{DL:DRP1}. Another noteworthy issue is the stuck of backpropagation signals since the neurons can be seen act as resistance, which could drastically chock the gradient from flowing further backwards if many such units in the frontal layers are in dorment state. Topological counter measures has been proposed along the way, such as residual learning based short circuiting \cite{DL:DRL1} to help the flow of gradient, the companion trainers \cite{DL:DSN1} to promote uniformed evolution, and the conglomeration of more than one the topological tweaks through implict ensembling \cite{DL:SWP1}, all of which aim to prevent part of of network, especially those of lower depth from stagnation. Asides from structure improvement, \cite{DL:Relu2} also proposed a more general form of ReLU called PRelu, such that the negative part of the activation allows non-constant, thus avoiding 0 gradient. These augmentations, including the very adaptation of ReLU itself, all strictly follows the central doctrine that ``deeper is better'', which might be approximated or even out performed in domains other than imaging, acoustic, and texture based classification, by shallower networks of alternative structure components.

% Head 1
\section{Proposal}
Motivated by \cite{DL:Relu2}, which grants the negtive part of the ReLU with non-zero gradient to prevent gradient from completely diminishing, we propose a adaptive form of softplus (ASP) to compare with ReLU. ASP is differentiable over $\mathcal{R}$, with trainable parameters to control its smoothness. In such way it allow the data to decide the proper shape of the activation unit, which can be as sharp as the mainstream ReLU, or as smooth as the softplus, while retaining advantages like asymmetry and high gradient upon large positive input that come with Relu.

In this study we propose to expriment networks that is not very deep under the question other than common benchmarks. One candidate domain is the disease prediction with genome that may favor smaller networks, since the data itself is extremely sparse thanks to the deep sequencing technology, yet highly correlated due to linkage disequilibrium. We would also explore the shallow network's performance with semi-supervised encoding tasks (i.e., non-linear PCA).

We would assess the property of the proposed ASP in comparison with Relu via the following criteria 
\begin{itemize}
\item generalization error and convergence rate;
\item The network sparsity under fixed or flexible smoothing parameter.
\end{itemize}
These criteria will be measured under the following scenarios
\begin{itemize}
\item Image classification (reference);
\item Genome based prediction;
\item Genome auto-endocing.
\end{itemize}
For these project, we use fully connected networks.

% Bibliography
\bibliographystyle{ACM-Reference-Format}
\bibliography{ref}

\end{document}
